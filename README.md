# LLM-EDA

1. A data-centric chip design agent framework for Verilog code generation [https://dl.acm.org/doi/10.1145/3727980].

   ACM Trans 2025

  Recent advances in large language models (LLMs) have demonstrated significant potential for automated hardware description language (HDL) code generation from high-level specifications. However, two critical challenges limit further progress in this domain: the scarcity of quality Verilog training data and the inability of current approaches to generate RTL code optimized for power, performance, and area (PPA) metrics.
This paper presents a comprehensive data-centric framework that addresses these limitations through innovations in both pre-fine-tuning data preparation and after-fine-tuning optimization strategies. In the pre-fine-tuning phase, we tackle the data scarcity problem with an automated design-data augmentation framework that generates high-volume, high-quality natural language specifications aligned with corresponding Verilog code and EDA scripts. Our approach creates a complete RTL-level feedback loop by augmenting EDA scripts, RTL code, and EDA tool feedback. In the after-fine-tuning phase, we focus on generating PPA-aware RTL code through a novel search and prompt framework. Our approach implements iterative filtering and selection of LLM-generated Verilog variants while providing high-quality predefined prompts, including composition and interface specifications.
To evaluate the effectiveness of our data augmentation method, we fine-tune Llama 2-13B and Llama 2-7B models using the dataset generated by our augmentation framework. The results demonstrate a significant improvement in the Verilog generation tasks with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark. Our 13B model has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data. Additionally, to evaluate the effectiveness of the our agent framework, we compare the PPA on the GPT-3.5, where the results show that the agent refined RTL code can have a better quality than the generated RTL code only with GPT-3.5.

  大型语言模型（LLMS）的最新进展显示出从高级规格中生成自动硬件说明语言（HDL）代码的巨大潜力。但是，两个关键挑战限制了该领域的进一步进展：质量Verilog培训数据的稀缺性以及目前无法为功率，性能和区域（PPA）指标优化的RTL代码的方法。
本文提出了一个全面的以数据为中心的框架，该框架通过预先调整数据准备和提前调整优化策略的创新来解决这些限制。在预先调整阶段，我们通过自动设计数据扩展框架解决数据稀缺问题，该框架生成了与相应的Verilog Code和EDA脚本一致的高体积的高质量自然语言规格。我们的方法通过增强EDA脚本，RTL代码和EDA工具反馈来创建完整的RTL级反馈循环。在添加后调整阶段，我们专注于通过新颖的搜索和及时框架生成PPA感知的RTL代码。我们的方法实现了LLM生成的Verilog变体的迭代过滤和选择，同时提供了高质量的预定义提示，包括组成和接口规格。
为了评估我们的数据增强方法的有效性，我们使用我们的增强框架生成的数据集微调了Llama 2-13B和Llama 2-7B模型。结果表明，LLMS的Verilog生成任务有了显着改善。此外，Verilog生成的准确性超过了当前最新的开源Verilog生成模型的准确性，相同基准的准确性从58.8％增加到70.6％。与GPT-3.5相比，我们的13B模型在Verilog生成中的传递率有所提高，并且EDA脚本（即SiliconCompiler）的生成效果均超过了200 EDA脚本数据。此外，为了评估我们的代理框架的有效性，我们比较了GPT-3.5上的PPA，其中结果表明，代理完善的RTL代码可以比仅使用GPT-3.5生成的RTL代码具有更好的质量。
